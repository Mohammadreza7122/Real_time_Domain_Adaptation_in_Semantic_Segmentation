{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB7Ncw_tyGu9",
        "outputId": "560c2079-3267-43b9-c43e-fd9ba040457d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=d5fbb9dfdd2cb579f5464d1148d99824d354564d184bc0b45b23fe488432df92\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=6600b090cf343b9ae2e0a6e17fd65b675a92fe331de7820766007d84495247d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, nvidia-cusolver-cu12, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 portalocker-2.10.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "! pip install torch fvcore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "class resnet18(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet18(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "class resnet101(torch.nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        self.features = models.resnet101(pretrained=pretrained)\n",
        "        self.conv1 = self.features.conv1\n",
        "        self.bn1 = self.features.bn1\n",
        "        self.relu = self.features.relu\n",
        "        self.maxpool1 = self.features.maxpool\n",
        "        self.layer1 = self.features.layer1\n",
        "        self.layer2 = self.features.layer2\n",
        "        self.layer3 = self.features.layer3\n",
        "        self.layer4 = self.features.layer4\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        x = self.relu(self.bn1(x))\n",
        "        x = self.maxpool1(x)\n",
        "        feature1 = self.layer1(x)  # 1 / 4\n",
        "        feature2 = self.layer2(feature1)  # 1 / 8\n",
        "        feature3 = self.layer3(feature2)  # 1 / 16\n",
        "        feature4 = self.layer4(feature3)  # 1 / 32\n",
        "        # global average pooling to build tail\n",
        "        tail = torch.mean(feature4, 3, keepdim=True)\n",
        "        tail = torch.mean(tail, 2, keepdim=True)\n",
        "        return feature3, feature4, tail\n",
        "\n",
        "\n",
        "def build_contextpath(name):\n",
        "    model = {\n",
        "        'resnet18': resnet18(pretrained=True),\n",
        "        'resnet101': resnet101(pretrained=True)\n",
        "    }\n",
        "    return model[name]"
      ],
      "metadata": {
        "id": "W64TA7TiyMUf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "\n",
        "class ConvBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
        "                               stride=stride, padding=padding, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv1(input)\n",
        "        return self.relu(self.bn(x))\n",
        "\n",
        "\n",
        "class Spatial_path(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.convblock1 = ConvBlock(in_channels=3, out_channels=64)\n",
        "        self.convblock2 = ConvBlock(in_channels=64, out_channels=128)\n",
        "        self.convblock3 = ConvBlock(in_channels=128, out_channels=256)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.convblock1(input)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.convblock3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionRefinementModule(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.in_channels = in_channels\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input):\n",
        "        # global average pooling\n",
        "        x = self.avgpool(input)\n",
        "        assert self.in_channels == x.size(1), 'in_channels and out_channels should all be {}'.format(x.size(1))\n",
        "        x = self.conv(x)\n",
        "        x = self.sigmoid(self.bn(x))\n",
        "        # x = self.sigmoid(x)\n",
        "        # channels of input and x should be same\n",
        "        x = torch.mul(input, x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FeatureFusionModule(torch.nn.Module):\n",
        "    def __init__(self, num_classes, in_channels):\n",
        "        super().__init__()\n",
        "        # self.in_channels = input_1.channels + input_2.channels\n",
        "        # resnet101 3328 = 256(from spatial path) + 1024(from context path) + 2048(from context path)\n",
        "        # resnet18  1024 = 256(from spatial path) + 256(from context path) + 512(from context path)\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.convblock = ConvBlock(in_channels=self.in_channels, out_channels=num_classes, stride=1)\n",
        "        self.conv1 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(num_classes, num_classes, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "\n",
        "    def forward(self, input_1, input_2):\n",
        "        x = torch.cat((input_1, input_2), dim=1)\n",
        "        assert self.in_channels == x.size(1), 'in_channels of ConvBlock should be {}'.format(x.size(1))\n",
        "        feature = self.convblock(x)\n",
        "        x = self.avgpool(feature)\n",
        "\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.sigmoid(self.conv2(x))\n",
        "        x = torch.mul(feature, x)\n",
        "        x = torch.add(x, feature)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BiSeNet(torch.nn.Module):\n",
        "    def __init__(self, num_classes, context_path):\n",
        "        super().__init__()\n",
        "        # build spatial path\n",
        "        self.saptial_path = Spatial_path()\n",
        "\n",
        "        # build context path\n",
        "        self.context_path = build_contextpath(name=context_path)\n",
        "\n",
        "        # build attention refinement module  for resnet 101\n",
        "        if context_path == 'resnet101':\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(1024, 1024)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(2048, 2048)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=1024, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=2048, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 3328)\n",
        "\n",
        "        elif context_path == 'resnet18':\n",
        "            # build attention refinement module  for resnet 18\n",
        "            self.attention_refinement_module1 = AttentionRefinementModule(256, 256)\n",
        "            self.attention_refinement_module2 = AttentionRefinementModule(512, 512)\n",
        "            # supervision block\n",
        "            self.supervision1 = nn.Conv2d(in_channels=256, out_channels=num_classes, kernel_size=1)\n",
        "            self.supervision2 = nn.Conv2d(in_channels=512, out_channels=num_classes, kernel_size=1)\n",
        "            # build feature fusion module\n",
        "            self.feature_fusion_module = FeatureFusionModule(num_classes, 1024)\n",
        "        else:\n",
        "            print('Error: unspport context_path network \\n')\n",
        "\n",
        "        # build final convolution\n",
        "        self.conv = nn.Conv2d(in_channels=num_classes, out_channels=num_classes, kernel_size=1)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "        self.mul_lr = []\n",
        "        self.mul_lr.append(self.saptial_path)\n",
        "        self.mul_lr.append(self.attention_refinement_module1)\n",
        "        self.mul_lr.append(self.attention_refinement_module2)\n",
        "        self.mul_lr.append(self.supervision1)\n",
        "        self.mul_lr.append(self.supervision2)\n",
        "        self.mul_lr.append(self.feature_fusion_module)\n",
        "        self.mul_lr.append(self.conv)\n",
        "\n",
        "    def init_weight(self):\n",
        "        for name, m in self.named_modules():\n",
        "            if 'context_path' not in name:\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    m.eps = 1e-5\n",
        "                    m.momentum = 0.1\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # output of spatial path\n",
        "        sx = self.saptial_path(input)\n",
        "\n",
        "        # output of context path\n",
        "        cx1, cx2, tail = self.context_path(input)\n",
        "        cx1 = self.attention_refinement_module1(cx1)\n",
        "        cx2 = self.attention_refinement_module2(cx2)\n",
        "        cx2 = torch.mul(cx2, tail)\n",
        "        # upsampling\n",
        "        cx1 = torch.nn.functional.interpolate(cx1, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx2 = torch.nn.functional.interpolate(cx2, size=sx.size()[-2:], mode='bilinear')\n",
        "        cx = torch.cat((cx1, cx2), dim=1)\n",
        "\n",
        "        if self.training == True:\n",
        "            cx1_sup = self.supervision1(cx1)\n",
        "            cx2_sup = self.supervision2(cx2)\n",
        "            cx1_sup = torch.nn.functional.interpolate(cx1_sup, size=input.size()[-2:], mode='bilinear')\n",
        "            cx2_sup = torch.nn.functional.interpolate(cx2_sup, size=input.size()[-2:], mode='bilinear')\n",
        "\n",
        "        # output of feature fusion module\n",
        "        result = self.feature_fusion_module(sx, cx)\n",
        "\n",
        "        # upsampling\n",
        "        result = torch.nn.functional.interpolate(result, scale_factor=8, mode='bilinear')\n",
        "        result = self.conv(result)\n",
        "\n",
        "        if self.training == True:\n",
        "            return result, cx1_sup, cx2_sup\n",
        "\n",
        "        return result"
      ],
      "metadata": {
        "id": "KyX9mNr5yMXL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "model_path = '/content/drive/MyDrive/MLDLMLDL/best_model_2b.pth'\n",
        "num_classes = 19\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "model = BiSeNet(num_classes=num_classes, context_path='resnet18').to(device)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "height = 512\n",
        "width = 1024\n",
        "\n",
        "image = torch.zeros((1, 3, height, width)).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1bR_qBOyMZi",
        "outputId": "3d6ca000-1c13-4775-d1fe-2586255fc963"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 211MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n",
            "100%|██████████| 171M/171M [00:00<00:00, 221MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flops = FlopCountAnalysis(model, image)\n",
        "print(flop_count_table(flops))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcIXqfzFyMcA",
        "outputId": "3ceb63d0-a74f-4f2d-a330-86d5979ded40"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| module                                      | #parameters or shape   | #flops     |\n",
            "|:--------------------------------------------|:-----------------------|:-----------|\n",
            "| model                                       | 12.582M                | 25.78G     |\n",
            "|  saptial_path                               |  0.371M                |  5.088G    |\n",
            "|   saptial_path.convblock1                   |   1.856K               |   0.243G   |\n",
            "|    saptial_path.convblock1.conv1            |    1.728K              |    0.226G  |\n",
            "|    saptial_path.convblock1.bn               |    0.128K              |    16.777M |\n",
            "|   saptial_path.convblock2                   |   73.984K              |   2.424G   |\n",
            "|    saptial_path.convblock2.conv1            |    73.728K             |    2.416G  |\n",
            "|    saptial_path.convblock2.bn               |    0.256K              |    8.389M  |\n",
            "|   saptial_path.convblock3                   |   0.295M               |   2.42G    |\n",
            "|    saptial_path.convblock3.conv1            |    0.295M              |    2.416G  |\n",
            "|    saptial_path.convblock3.bn               |    0.512K              |    4.194M  |\n",
            "|  context_path.features                      |  11.69M                |  19.002G   |\n",
            "|   context_path.features.conv1               |   9.408K               |   1.233G   |\n",
            "|    context_path.features.conv1.weight       |    (64, 3, 7, 7)       |            |\n",
            "|   context_path.features.bn1                 |   0.128K               |   16.777M  |\n",
            "|    context_path.features.bn1.weight         |    (64,)               |            |\n",
            "|    context_path.features.bn1.bias           |    (64,)               |            |\n",
            "|   context_path.features.layer1              |   0.148M               |   4.849G   |\n",
            "|    context_path.features.layer1.0           |    73.984K             |    2.424G  |\n",
            "|    context_path.features.layer1.1           |    73.984K             |    2.424G  |\n",
            "|   context_path.features.layer2              |   0.526M               |   4.305G   |\n",
            "|    context_path.features.layer2.0           |    0.23M               |    1.885G  |\n",
            "|    context_path.features.layer2.1           |    0.295M              |    2.42G   |\n",
            "|   context_path.features.layer3              |   2.1M                 |   4.3G     |\n",
            "|    context_path.features.layer3.0           |    0.919M              |    1.882G  |\n",
            "|    context_path.features.layer3.1           |    1.181M              |    2.418G  |\n",
            "|   context_path.features.layer4              |   8.394M               |   4.298G   |\n",
            "|    context_path.features.layer4.0           |    3.673M              |    1.881G  |\n",
            "|    context_path.features.layer4.1           |    4.721M              |    2.417G  |\n",
            "|   context_path.features.fc                  |   0.513M               |            |\n",
            "|    context_path.features.fc.weight          |    (1000, 512)         |            |\n",
            "|    context_path.features.fc.bias            |    (1000,)             |            |\n",
            "|  attention_refinement_module1               |  66.304K               |  0.59M     |\n",
            "|   attention_refinement_module1.conv         |   65.792K              |   65.536K  |\n",
            "|    attention_refinement_module1.conv.weight |    (256, 256, 1, 1)    |            |\n",
            "|    attention_refinement_module1.conv.bias   |    (256,)              |            |\n",
            "|   attention_refinement_module1.bn           |   0.512K               |   0.512K   |\n",
            "|    attention_refinement_module1.bn.weight   |    (256,)              |            |\n",
            "|    attention_refinement_module1.bn.bias     |    (256,)              |            |\n",
            "|   attention_refinement_module1.avgpool      |                        |   0.524M   |\n",
            "|  attention_refinement_module2               |  0.264M                |  0.525M    |\n",
            "|   attention_refinement_module2.conv         |   0.263M               |   0.262M   |\n",
            "|    attention_refinement_module2.conv.weight |    (512, 512, 1, 1)    |            |\n",
            "|    attention_refinement_module2.conv.bias   |    (512,)              |            |\n",
            "|   attention_refinement_module2.bn           |   1.024K               |   1.024K   |\n",
            "|    attention_refinement_module2.bn.weight   |    (512,)              |            |\n",
            "|    attention_refinement_module2.bn.bias     |    (512,)              |            |\n",
            "|   attention_refinement_module2.avgpool      |                        |   0.262M   |\n",
            "|  supervision1                               |  4.883K                |            |\n",
            "|   supervision1.weight                       |   (19, 256, 1, 1)      |            |\n",
            "|   supervision1.bias                         |   (19,)                |            |\n",
            "|  supervision2                               |  9.747K                |            |\n",
            "|   supervision2.weight                       |   (19, 512, 1, 1)      |            |\n",
            "|   supervision2.bias                         |   (19,)                |            |\n",
            "|  feature_fusion_module                      |  0.176M                |  1.435G    |\n",
            "|   feature_fusion_module.convblock           |   0.175M               |   1.435G   |\n",
            "|    feature_fusion_module.convblock.conv1    |    0.175M              |    1.434G  |\n",
            "|    feature_fusion_module.convblock.bn       |    38                  |    0.311M  |\n",
            "|   feature_fusion_module.conv1               |   0.38K                |   0.361K   |\n",
            "|    feature_fusion_module.conv1.weight       |    (19, 19, 1, 1)      |            |\n",
            "|    feature_fusion_module.conv1.bias         |    (19,)               |            |\n",
            "|   feature_fusion_module.conv2               |   0.38K                |   0.361K   |\n",
            "|    feature_fusion_module.conv2.weight       |    (19, 19, 1, 1)      |            |\n",
            "|    feature_fusion_module.conv2.bias         |    (19,)               |            |\n",
            "|   feature_fusion_module.avgpool             |                        |   0.156M   |\n",
            "|  conv                                       |  0.38K                 |  0.189G    |\n",
            "|   conv.weight                               |   (19, 19, 1, 1)       |            |\n",
            "|   conv.bias                                 |   (19,)                |            |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from statistics import mean, stdev\n",
        "\n",
        "iterations = 1000\n",
        "latency = []\n",
        "FPS = []\n",
        "\n",
        "for i in range(iterations):\n",
        "    start = time.time()\n",
        "    output = model(image)\n",
        "    end = time.time()\n",
        "\n",
        "    latencyi = end - start\n",
        "\n",
        "    if latencyi != 0.0:\n",
        "        latency.append(latencyi)\n",
        "        FPSi = 1/latencyi\n",
        "        FPS.append(FPSi)\n",
        "\n",
        "mean_latency = np.mean(latency) * 1000\n",
        "std_latency = np.std(latency) * 1000\n",
        "mean_fPS = np.mean(FPS)\n",
        "std_fPS = np.std(FPS)\n",
        "\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "print(f\"Std Latency: {std_latency:.2f} ms\")\n",
        "print(f\"Mean FPS: {mean_fPS:.2f}\")\n",
        "print(f\"Std FPS: {std_fPS:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWzOcyd75SJw",
        "outputId": "90e1ea1c-97fa-490f-9b12-ced8e5899224"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Latency: 5.99 ms\n",
            "Std Latency: 0.34 ms\n",
            "Mean FPS: 167.52\n",
            "Std FPS: 10.28\n"
          ]
        }
      ]
    }
  ]
}