{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pcm21k4QYhU",
        "outputId": "dcd25677-22e8-40bd-85c9-13b6bb6242a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fvcore) (1.25.2)\n",
            "Collecting yacs>=0.1.6 (from fvcore)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore) (2.4.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from fvcore) (9.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore)\n",
            "  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=497e8a3c1571faf2885c8b37b72e077056e7e36bf9bfc5671d6fbd8497cccf63\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=ad157094f978efebbf5445dd2790aa42c4da8e9f232bdd7dcda7fd0cc8bdcda1\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, nvidia-cusolver-cu12, fvcore\n",
            "Successfully installed fvcore-0.1.5.post20221221 iopath-0.1.10 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 portalocker-2.10.0 yacs-0.1.8\n"
          ]
        }
      ],
      "source": [
        "! pip install torch fvcore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model components and the model class as provided\n",
        "affine_par = True\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes, affine=affine_par)\n",
        "        for i in self.bn1.parameters():\n",
        "            i.requires_grad = False\n",
        "        padding = dilation\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
        "                               padding=padding, bias=False, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, affine=affine_par)\n",
        "        for i in self.bn2.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4, affine=affine_par)\n",
        "        for i in self.bn3.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ClassifierModule(nn.Module):\n",
        "    def __init__(self, inplanes, dilation_series, padding_series, num_classes):\n",
        "        super(ClassifierModule, self).__init__()\n",
        "        self.conv2d_list = nn.ModuleList()\n",
        "        for dilation, padding in zip(dilation_series, padding_series):\n",
        "            self.conv2d_list.append(\n",
        "                nn.Conv2d(inplanes, num_classes, kernel_size=3, stride=1, padding=padding,\n",
        "                          dilation=dilation, bias=True))\n",
        "\n",
        "        for m in self.conv2d_list:\n",
        "            m.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv2d_list[0](x)\n",
        "        for i in range(len(self.conv2d_list) - 1):\n",
        "            out += self.conv2d_list[i + 1](x)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNetMulti(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        self.inplanes = 64\n",
        "        super(ResNetMulti, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64, affine=affine_par)\n",
        "        for i in self.bn1.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)  # change\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=1, dilation=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=4)\n",
        "        self.layer6 = ClassifierModule(2048, [6, 12, 18, 24], [6, 12, 18, 24], num_classes)\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                m.weight.data.normal_(0, 0.01)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
        "        downsample = None\n",
        "        if (stride != 1\n",
        "                or self.inplanes != planes * block.expansion\n",
        "                or dilation == 2\n",
        "                or dilation == 4):\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion, affine=affine_par))\n",
        "        for i in downsample._modules['1'].parameters():\n",
        "            i.requires_grad = False\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.inplanes, planes, stride, dilation=dilation, downsample=downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes, dilation=dilation))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, H, W = x.size()\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer6(x)\n",
        "\n",
        "        x = torch.nn.functional.interpolate(x, size=(H, W), mode='bilinear')\n",
        "\n",
        "        if self.training == True:\n",
        "            return x, None, None\n",
        "\n",
        "        return x\n",
        "\n",
        "    def get_1x_lr_params_no_scale(self):\n",
        "        \"\"\"\n",
        "        This generator returns all the parameters of the net except for\n",
        "        the last classification layer. Note that for each batchnorm layer,\n",
        "        requires_grad is set to False in deeplab_resnet.py, therefore this function does not return\n",
        "        any batchnorm parameter\n",
        "        \"\"\"\n",
        "        b = []\n",
        "\n",
        "        b.append(self.conv1)\n",
        "        b.append(self.bn1)\n",
        "        b.append(self.layer1)\n",
        "        b.append(self.layer2)\n",
        "        b.append(self.layer3)\n",
        "        b.append(self.layer4)\n",
        "\n",
        "        for i in range(len(b)):\n",
        "            for j in b[i].modules():\n",
        "                jj = 0\n",
        "                for k in j.parameters():\n",
        "                    jj += 1\n",
        "                    if k.requires_grad:\n",
        "                        yield k\n",
        "\n",
        "    def get_10x_lr_params(self):\n",
        "        \"\"\"\n",
        "        This generator returns all the parameters for the last layer of the net,\n",
        "        which does the classification of pixel into classes\n",
        "        \"\"\"\n",
        "        b = []\n",
        "        if self.multi_level:\n",
        "            b.append(self.layer5.parameters())\n",
        "        b.append(self.layer6.parameters())\n",
        "\n",
        "        for j in range(len(b)):\n",
        "            for i in b[j]:\n",
        "                yield i\n",
        "\n",
        "    def optim_parameters(self, lr):\n",
        "        return [{'params': self.get_1x_lr_params_no_scale(), 'lr': lr},\n",
        "                {'params': self.get_10x_lr_params(), 'lr': 10 * lr}]\n",
        "\n",
        "def get_deeplab_v2(num_classes=19, pretrain=True, pretrain_model_path='DeepLab_resnet_pretrained_imagenet.pth'):\n",
        "    model = ResNetMulti(Bottleneck, [3, 4, 23, 3], num_classes)\n",
        "\n",
        "    # Pretraining loading\n",
        "    if pretrain:\n",
        "        print('Deeplab pretraining loading...')\n",
        "        saved_state_dict = torch.load(pretrain_model_path)\n",
        "\n",
        "        new_params = model.state_dict().copy()\n",
        "        for i in saved_state_dict:\n",
        "            i_parts = i.split('.')\n",
        "            new_params['.'.join(i_parts[1:])] = saved_state_dict[i]\n",
        "        model.load_state_dict(new_params, strict=False)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_vPhOWb1uLXs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fvcore.nn import FlopCountAnalysis, flop_count_table\n",
        "\n",
        "model_path = '/content/drive/MyDrive/MLDLMLDL/best_model_2a.pth'\n",
        "num_classes = 19\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = get_deeplab_v2(num_classes=num_classes, pretrain=False).to(device)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "height = 512\n",
        "width = 1024\n",
        "\n",
        "image = torch.zeros((1, 3, height, width)).to(device)"
      ],
      "metadata": {
        "id": "gR-rHIVcuLZq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flops = FlopCountAnalysis(model, image)\n",
        "print(flop_count_table(flops))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEdoUfVpuLcH",
        "outputId": "80458bfa-bd0e-4169-8479-30bc27f5cbc2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| module                         | #parameters or shape   | #flops     |\n",
            "|:-------------------------------|:-----------------------|:-----------|\n",
            "| model                          | 43.901M                | 0.375T     |\n",
            "|  conv1                         |  9.408K                |  1.233G    |\n",
            "|   conv1.weight                 |   (64, 3, 7, 7)        |            |\n",
            "|  bn1                           |  0.128K                |  16.777M   |\n",
            "|   bn1.weight                   |   (64,)                |            |\n",
            "|   bn1.bias                     |   (64,)                |            |\n",
            "|  layer1                        |  0.216M                |  7.155G    |\n",
            "|   layer1.0                     |   75.008K              |   2.487G   |\n",
            "|    layer1.0.conv1              |    4.096K              |    0.136G  |\n",
            "|    layer1.0.bn1                |    0.128K              |    4.244M  |\n",
            "|    layer1.0.conv2              |    36.864K             |    1.222G  |\n",
            "|    layer1.0.bn2                |    0.128K              |    4.244M  |\n",
            "|    layer1.0.conv3              |    16.384K             |    0.543G  |\n",
            "|    layer1.0.bn3                |    0.512K              |    16.974M |\n",
            "|    layer1.0.downsample         |    16.896K             |    0.56G   |\n",
            "|   layer1.1                     |   70.4K                |   2.334G   |\n",
            "|    layer1.1.conv1              |    16.384K             |    0.543G  |\n",
            "|    layer1.1.bn1                |    0.128K              |    4.244M  |\n",
            "|    layer1.1.conv2              |    36.864K             |    1.222G  |\n",
            "|    layer1.1.bn2                |    0.128K              |    4.244M  |\n",
            "|    layer1.1.conv3              |    16.384K             |    0.543G  |\n",
            "|    layer1.1.bn3                |    0.512K              |    16.974M |\n",
            "|   layer1.2                     |   70.4K                |   2.334G   |\n",
            "|    layer1.2.conv1              |    16.384K             |    0.543G  |\n",
            "|    layer1.2.bn1                |    0.128K              |    4.244M  |\n",
            "|    layer1.2.conv2              |    36.864K             |    1.222G  |\n",
            "|    layer1.2.bn2                |    0.128K              |    4.244M  |\n",
            "|    layer1.2.conv3              |    16.384K             |    0.543G  |\n",
            "|    layer1.2.bn3                |    0.512K              |    16.974M |\n",
            "|  layer2                        |  1.22M                 |  10.226G   |\n",
            "|   layer2.0                     |   0.379M               |   3.181G   |\n",
            "|    layer2.0.conv1              |    32.768K             |    0.275G  |\n",
            "|    layer2.0.bn1                |    0.256K              |    2.147M  |\n",
            "|    layer2.0.conv2              |    0.147M              |    1.236G  |\n",
            "|    layer2.0.bn2                |    0.256K              |    2.147M  |\n",
            "|    layer2.0.conv3              |    65.536K             |    0.55G   |\n",
            "|    layer2.0.bn3                |    1.024K              |    8.586M  |\n",
            "|    layer2.0.downsample         |    0.132M              |    1.108G  |\n",
            "|   layer2.1                     |   0.28M                |   2.348G   |\n",
            "|    layer2.1.conv1              |    65.536K             |    0.55G   |\n",
            "|    layer2.1.bn1                |    0.256K              |    2.147M  |\n",
            "|    layer2.1.conv2              |    0.147M              |    1.236G  |\n",
            "|    layer2.1.bn2                |    0.256K              |    2.147M  |\n",
            "|    layer2.1.conv3              |    65.536K             |    0.55G   |\n",
            "|    layer2.1.bn3                |    1.024K              |    8.586M  |\n",
            "|   layer2.2                     |   0.28M                |   2.348G   |\n",
            "|    layer2.2.conv1              |    65.536K             |    0.55G   |\n",
            "|    layer2.2.bn1                |    0.256K              |    2.147M  |\n",
            "|    layer2.2.conv2              |    0.147M              |    1.236G  |\n",
            "|    layer2.2.bn2                |    0.256K              |    2.147M  |\n",
            "|    layer2.2.conv3              |    65.536K             |    0.55G   |\n",
            "|    layer2.2.bn3                |    1.024K              |    8.586M  |\n",
            "|   layer2.3                     |   0.28M                |   2.348G   |\n",
            "|    layer2.3.conv1              |    65.536K             |    0.55G   |\n",
            "|    layer2.3.bn1                |    0.256K              |    2.147M  |\n",
            "|    layer2.3.conv2              |    0.147M              |    1.236G  |\n",
            "|    layer2.3.bn2                |    0.256K              |    2.147M  |\n",
            "|    layer2.3.conv3              |    65.536K             |    0.55G   |\n",
            "|    layer2.3.bn3                |    1.024K              |    8.586M  |\n",
            "|  layer3                        |  26.09M                |  0.219T    |\n",
            "|   layer3.0                     |   1.512M               |   12.682G  |\n",
            "|    layer3.0.conv1              |    0.131M              |    1.099G  |\n",
            "|    layer3.0.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.0.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.0.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.0.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.0.bn3                |    2.048K              |    17.172M |\n",
            "|    layer3.0.downsample         |    0.526M              |    4.413G  |\n",
            "|   layer3.1                     |   1.117M               |   9.368G   |\n",
            "|    layer3.1.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.1.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.1.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.1.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.1.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.1.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.2                     |   1.117M               |   9.368G   |\n",
            "|    layer3.2.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.2.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.2.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.2.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.2.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.2.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.3                     |   1.117M               |   9.368G   |\n",
            "|    layer3.3.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.3.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.3.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.3.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.3.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.3.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.4                     |   1.117M               |   9.368G   |\n",
            "|    layer3.4.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.4.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.4.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.4.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.4.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.4.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.5                     |   1.117M               |   9.368G   |\n",
            "|    layer3.5.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.5.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.5.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.5.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.5.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.5.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.6                     |   1.117M               |   9.368G   |\n",
            "|    layer3.6.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.6.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.6.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.6.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.6.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.6.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.7                     |   1.117M               |   9.368G   |\n",
            "|    layer3.7.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.7.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.7.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.7.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.7.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.7.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.8                     |   1.117M               |   9.368G   |\n",
            "|    layer3.8.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.8.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.8.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.8.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.8.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.8.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.9                     |   1.117M               |   9.368G   |\n",
            "|    layer3.9.conv1              |    0.262M              |    2.198G  |\n",
            "|    layer3.9.bn1                |    0.512K              |    4.293M  |\n",
            "|    layer3.9.conv2              |    0.59M               |    4.946G  |\n",
            "|    layer3.9.bn2                |    0.512K              |    4.293M  |\n",
            "|    layer3.9.conv3              |    0.262M              |    2.198G  |\n",
            "|    layer3.9.bn3                |    2.048K              |    17.172M |\n",
            "|   layer3.10                    |   1.117M               |   9.368G   |\n",
            "|    layer3.10.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.10.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.10.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.10.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.10.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.10.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.11                    |   1.117M               |   9.368G   |\n",
            "|    layer3.11.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.11.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.11.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.11.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.11.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.11.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.12                    |   1.117M               |   9.368G   |\n",
            "|    layer3.12.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.12.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.12.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.12.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.12.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.12.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.13                    |   1.117M               |   9.368G   |\n",
            "|    layer3.13.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.13.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.13.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.13.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.13.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.13.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.14                    |   1.117M               |   9.368G   |\n",
            "|    layer3.14.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.14.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.14.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.14.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.14.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.14.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.15                    |   1.117M               |   9.368G   |\n",
            "|    layer3.15.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.15.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.15.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.15.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.15.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.15.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.16                    |   1.117M               |   9.368G   |\n",
            "|    layer3.16.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.16.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.16.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.16.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.16.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.16.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.17                    |   1.117M               |   9.368G   |\n",
            "|    layer3.17.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.17.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.17.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.17.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.17.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.17.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.18                    |   1.117M               |   9.368G   |\n",
            "|    layer3.18.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.18.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.18.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.18.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.18.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.18.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.19                    |   1.117M               |   9.368G   |\n",
            "|    layer3.19.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.19.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.19.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.19.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.19.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.19.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.20                    |   1.117M               |   9.368G   |\n",
            "|    layer3.20.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.20.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.20.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.20.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.20.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.20.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.21                    |   1.117M               |   9.368G   |\n",
            "|    layer3.21.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.21.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.21.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.21.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.21.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.21.bn3               |    2.048K              |    17.172M |\n",
            "|   layer3.22                    |   1.117M               |   9.368G   |\n",
            "|    layer3.22.conv1             |    0.262M              |    2.198G  |\n",
            "|    layer3.22.bn1               |    0.512K              |    4.293M  |\n",
            "|    layer3.22.conv2             |    0.59M               |    4.946G  |\n",
            "|    layer3.22.bn2               |    0.512K              |    4.293M  |\n",
            "|    layer3.22.conv3             |    0.262M              |    2.198G  |\n",
            "|    layer3.22.bn3               |    2.048K              |    17.172M |\n",
            "|  layer4                        |  14.965M               |  0.125T    |\n",
            "|   layer4.0                     |   6.04M                |   50.642G  |\n",
            "|    layer4.0.conv1              |    0.524M              |    4.396G  |\n",
            "|    layer4.0.bn1                |    1.024K              |    8.586M  |\n",
            "|    layer4.0.conv2              |    2.359M              |    19.783G |\n",
            "|    layer4.0.bn2                |    1.024K              |    8.586M  |\n",
            "|    layer4.0.conv3              |    1.049M              |    8.792G  |\n",
            "|    layer4.0.bn3                |    4.096K              |    34.345M |\n",
            "|    layer4.0.downsample         |    2.101M              |    17.619G |\n",
            "|   layer4.1                     |   4.463M               |   37.419G  |\n",
            "|    layer4.1.conv1              |    1.049M              |    8.792G  |\n",
            "|    layer4.1.bn1                |    1.024K              |    8.586M  |\n",
            "|    layer4.1.conv2              |    2.359M              |    19.783G |\n",
            "|    layer4.1.bn2                |    1.024K              |    8.586M  |\n",
            "|    layer4.1.conv3              |    1.049M              |    8.792G  |\n",
            "|    layer4.1.bn3                |    4.096K              |    34.345M |\n",
            "|   layer4.2                     |   4.463M               |   37.419G  |\n",
            "|    layer4.2.conv1              |    1.049M              |    8.792G  |\n",
            "|    layer4.2.bn1                |    1.024K              |    8.586M  |\n",
            "|    layer4.2.conv2              |    2.359M              |    19.783G |\n",
            "|    layer4.2.bn2                |    1.024K              |    8.586M  |\n",
            "|    layer4.2.conv3              |    1.049M              |    8.792G  |\n",
            "|    layer4.2.bn3                |    4.096K              |    34.345M |\n",
            "|  layer6.conv2d_list            |  1.401M                |  11.746G   |\n",
            "|   layer6.conv2d_list.0         |   0.35M                |   2.936G   |\n",
            "|    layer6.conv2d_list.0.weight |    (19, 2048, 3, 3)    |            |\n",
            "|    layer6.conv2d_list.0.bias   |    (19,)               |            |\n",
            "|   layer6.conv2d_list.1         |   0.35M                |   2.936G   |\n",
            "|    layer6.conv2d_list.1.weight |    (19, 2048, 3, 3)    |            |\n",
            "|    layer6.conv2d_list.1.bias   |    (19,)               |            |\n",
            "|   layer6.conv2d_list.2         |   0.35M                |   2.936G   |\n",
            "|    layer6.conv2d_list.2.weight |    (19, 2048, 3, 3)    |            |\n",
            "|    layer6.conv2d_list.2.bias   |    (19,)               |            |\n",
            "|   layer6.conv2d_list.3         |   0.35M                |   2.936G   |\n",
            "|    layer6.conv2d_list.3.weight |    (19, 2048, 3, 3)    |            |\n",
            "|    layer6.conv2d_list.3.bias   |    (19,)               |            |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from statistics import mean, stdev\n",
        "\n",
        "iterations = 1000\n",
        "latency = []\n",
        "FPS = []\n",
        "\n",
        "for i in range(iterations):\n",
        "    start = time.time()\n",
        "    output = model(image)\n",
        "    end = time.time()\n",
        "\n",
        "    latencyi = end - start\n",
        "\n",
        "    if latencyi != 0.0:\n",
        "        latency.append(latencyi)\n",
        "        FPSi = 1/latencyi\n",
        "        FPS.append(FPSi)\n",
        "\n",
        "mean_latency = np.mean(latency) * 1000\n",
        "std_latency = np.std(latency) * 1000\n",
        "mean_fPS = np.mean(FPS)\n",
        "std_fPS = np.std(FPS)\n",
        "\n",
        "print(f\"Mean Latency: {mean_latency:.2f} ms\")\n",
        "print(f\"Std Latency: {std_latency:.2f} ms\")\n",
        "print(f\"Mean FPS: {mean_fPS:.2f}\")\n",
        "print(f\"Std FPS: {std_fPS:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnHd65-6uLez",
        "outputId": "6b58f4f7-e4b0-4467-9c3e-e680de4275d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Latency: 72.94 ms\n",
            "Std Latency: 2.85 ms\n",
            "Mean FPS: 13.76\n",
            "Std FPS: 1.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S-c9sIRIzWg4"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}